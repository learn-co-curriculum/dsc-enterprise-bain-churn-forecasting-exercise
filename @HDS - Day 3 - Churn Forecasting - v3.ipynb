{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn forecasting and management\n",
    "\n",
    "https://www.kaggle.com/pangkw/telco-churn/version/3\n",
    "\n",
    "__Business case__: Our client is Triple Telco and they provide triple service communication services to consumers (landline phone, fast internet and TV service). Up until a few months ago they were the only triple play provider in their area. However, new provider entered their market and they've seen a dramatic increase in customers who \"churn\" (i.e. leave).\n",
    "\n",
    "They hired Bain to help identify drivers of churn and recommend actions to prevent it.\n",
    "\n",
    "__Note: This notebook will not be shared with trainees__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 1\n",
    "\n",
    "* Load the necessary libraries\n",
    "* Load the data\n",
    "* Explore data â€“ what types, any obvious issues\n",
    "* Run mechanical feature engineering (longest part)\n",
    "* Split data\n",
    "* Run logistic regression\n",
    "* Explore coefficients\n",
    "* Deep dive on the most predictive data\n",
    "* Clean-up data (if necessary)\n",
    "* Re-run logistic regression (if necessary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// Ensure our output is sufficiently large\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emnsure all charts plotted automatically\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Don't truncate dataframe displays columwise\n",
    "pd.set_option('display.max_columns', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer Data: Load the data\n",
    "\n",
    "You load the CSV data into a Pandas object. This is a common Python library to work with data in row/column format, like .csv files.\n",
    "\n",
    "Print out top 10 rows just to get a feel for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__You should see:__\n",
    "\n",
    "* 3333 rows of data across all 35 columns (different variables)\n",
    "* lots of rows have text data that needs to be converted into numerical data, ideally one-hot encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data: \n",
    "\n",
    "* Check  for nulls\n",
    "* Check for outliers on data that is numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for nulls and drop if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #First replace all empty strings with NAN\n",
    "\n",
    "\n",
    "# Check for any null values\n",
    "\n",
    "\n",
    "display(df.shape)\n",
    "df = df.dropna()\n",
    "display(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see__:\n",
    "\n",
    "* We have only 5 null values\n",
    " * Think OK to drop\n",
    " * In real world, you'd want to document this AND maybe even try to impute the missing number (since only missing Total Reveneu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for outliers in numerical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see__:\n",
    "* Nothing too suspicious, think can assume data is clean\n",
    "* Revenue is not a float or int - we should fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalRevenue']= pd.to_numeric(df['TotalRevenue'], errors='coerce').fillna(0, downcast='infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanical Feature Engineering\n",
    "\n",
    "Set up the mapping for mechanical feature engineering step. Once you've completed this, take a look at how this is handled in the solution branch--there's a much quicker, cleaner way to do this! \n",
    "\n",
    "(Hint--think about how you could use list comprehensions for this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We use capitals for constant variables\n",
    "TARGET = 'Churn'\n",
    "ID =  'customerID'\n",
    "FEATURES = df.columns.tolist()\n",
    "\n",
    "# Remove customer id - it can't possibly have any predictive power\n",
    "FEATURES.remove(ID)\n",
    "#remove target from list of features\n",
    "FEATURES.remove(TARGET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# Lets verify that our splitting has the same distribution\n",
    "display(Markdown(\"__Split in training set__\"))\n",
    "display(pd.Categorical(y_train[TARGET]).describe())\n",
    "display(Markdown(\"__Split in test set__\"))\n",
    "display(pd.Categorical(y_test[TARGET]).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see__:\n",
    "\n",
    "* Frequency of No Chrun (0) and yes chrun (1) should be the same in both training and test set\n",
    "* There should be about 2,300 data-points in training set and about 1000 in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop model: Train and test base-line model\n",
    "To train and test the base-line model we look at the type of prediction problem we are dealing with. The target is yes or no churn which is a binary outome.\n",
    "As such our algorithm needs to be able to predict this. A logistic regression is an algorithm that allows to predict binary variable. \n",
    "\n",
    "We will:\n",
    "* Train the model using linear logistic regression\n",
    "* Test it against unseen data (i.e. test data)\n",
    "* Show accuracy on seen data and unseen data\n",
    "\n",
    "*Note for statistics geeks only: The implementation of the algorithm in Python's scikit-learn is not based on ordinary least square or maximum likelihood estimator, which would be the case in R or SAS. Specifically, it penalizes for complexity (called regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Logistic Regression Training\n",
    "linear_model = None\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "display(Markdown('###### Performance on training set'))\n",
    "\n",
    "# Get predictions for training set\n",
    "y_lr_train = None\n",
    "\n",
    "# Get predictions for testing set\n",
    "y_lr_test = None\n",
    "\n",
    "# Get test set probabilities\n",
    "y_lr_test_proba = None\n",
    "\n",
    "\n",
    "# See how well we did - get accuracy for training set\n",
    "lr_accuracy = None\n",
    "display('Linear model train set accuracy: {:.2f}%'.format(lr_accuracy*100))\n",
    "display(Markdown('Linear model predicion distribution:'))\n",
    "display(pd.Categorical(y_lr_test).describe())\n",
    "\n",
    "# Get accuracy for testing set\n",
    "lr_accuracy = None\n",
    "display('Linear model test set accuracy: {:.2f}%'.format(lr_accuracy*100))\n",
    "display(Markdown('Linear model predicion distribution:'))\n",
    "display(pd.Categorical(y_lr_test).describe())\n",
    "\n",
    "# Display confusion matrix\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_lr_test), \n",
    "             columns=['Predicted Not Churn', 'Predicted to Churn'],\n",
    "             index=['Actual Not Churn', 'Actual Churn']))\n",
    "\n",
    "# Calculate AUROC\n",
    "false_positive_rate, true_positive_rate, thresholds = None\n",
    "roc_auc = None\n",
    "display(Markdown('\\n\\nauROC is: {}'.format(roc_auc * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC chart\n",
    "plt.plot(false_positive_rate, true_positive_rate, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see__\n",
    "\n",
    "* This is pretty good - 86% accuracy (remember overall distribution is 85:15, so a little bit better than a coin-flip)\n",
    "* AUC is 90 - this is very good for a linear model\n",
    "\n",
    "* Let's see how these decisions are made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore coefficients\n",
    "\n",
    "Idea here is to see what the model picked-up to get better understanding why it's predicting churn.\n",
    "\n",
    "* Explore  intercept\n",
    "* Explore  coeficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intercept\n",
    "intercept = None\n",
    "\n",
    "# Get coefficients (same order as features fed into Logistic Regression) and build table\n",
    "named_coefs = None\n",
    "\n",
    "# put it in a nice dataframe and sort from most impact to least, sicne raw print-out is ugly\n",
    "display(\"Intercept is {}\".format(intercept[0]))\n",
    "display(named_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see__\n",
    "\n",
    "* list of weights on attributes that drive folks to churn (+ number) or not churn(- number)\n",
    "* Top five things making folks more likely to churn:\n",
    " * International plan - a bit odd - maybe we overcharged them a ton?\n",
    " * Contract_Month-to-month - makes a ton of sense, folks are ready to leave\n",
    " * Total revenue - seems customers who spend a ton, maybe competitors are cheaper\n",
    " * Customer service calls - makes a ton of sense\n",
    " * TotalDayMinutes - folks who talk a lot - that's bad - these are probably best customers\n",
    " \n",
    "    \n",
    "* Top five attributes contributing to not churning:\n",
    " * 2 year contract - makes sense - they can't leave without penalty\n",
    " * voicemailplan - folks who have voicemail with us - maybe they really like this feature\n",
    " * Multiple lines - maybe our competitors don't offer this feature?\n",
    " * tenure - folks who are with us for a long time, like to stay with us\n",
    " * Contract - one year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 2\n",
    "\n",
    "* Then let's run XGBoost model\n",
    "\n",
    "* Let's do a bit of feature engineering:\n",
    " * Maybe those who have lots of latency when they watch online tv/movies (i.e. share of high bandwidth mintues that are high latency)\n",
    " * Those who overpay for services\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First - let's run it as XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and train the model\n",
    "XG_model = None\n",
    "\n",
    "# Get test predictions\n",
    "y_predict = None\n",
    "\n",
    "# Get test prediction probabilities \n",
    "y_predict_proba = None\n",
    "\n",
    "# Calculate accuracy\n",
    "xg_accuracy = None\n",
    "display('XGBoost model test set accuracy: {:.4f}'.format(xg_accuracy))\n",
    "display('XGBoost model predicion distribution')\n",
    "display(pd.Categorical(y_predict).describe())\n",
    "\n",
    "# Display confusion matrix as a dataframe\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predict), \n",
    "             columns=['Predicted Not Churn', 'Predicted to Churn'],\n",
    "             index=['Actual Not Churn', 'Actual Churn']))\n",
    "\n",
    "# Calculate AUROC\n",
    "false_positive_rate, true_positive_rate, thresholds = None\n",
    "roc_auc = None\n",
    "display(\"auROC is: {}\".format(roc_auc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___What you should see:___\n",
    "\n",
    "* Impressive improvment\n",
    " * Accuracy up to 91% and auRoc to 95!\n",
    " \n",
    "__What is next__: Let's see feature importnace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC chart\n",
    "plt.plot(false_positive_rate, true_positive_rate, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's understand what's going on with features\n",
    "\n",
    "Create a dataframe of features and feature importances on the line below (check the solution branch if you're not sure about the syntax!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___What you should see:___\n",
    "\n",
    "* Seems features around time on the phone are dominating \n",
    "* Latency minutes drive some dissatisfaction too\n",
    "* Contract and tenure too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Engineering \n",
    "\n",
    "* first let's create a features\n",
    "* then explore it to see if it may be a signal\n",
    "* then re-run model with it\n",
    "\n",
    "Features:\n",
    "* Latency as share of traffic\n",
    "* Dollars paid  per service\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First up - Latency minutes - is there a signal here?\n",
    "\n",
    "* What we care about here is what proportion of time when download speeds are high (e.g. movie watching) also experiences high latency - to do that we'll need to divide high latency minutes by high bandwidth minutes\n",
    "* Since we scalarized these in our feature-engineered one, we need to work on original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latency = pd.DataFrame()\n",
    "df_latency['Churn'] = df['Churn'].copy()\n",
    "df_latency['TotalHighBandwidthMinutes'] = None\n",
    "df_latency['TotalHighLatencyMinutes'] = None\n",
    "df_latency['Latency_share'] = None\n",
    "df_latency['Latency_share'] = None\n",
    "\n",
    "# Now, plot a histogram on the line below\n",
    "\n",
    "latency_bins = [df_latency['Latency_share'].min(), .1, .2, .3, .4, df_latency['Latency_share'].max()]\n",
    "df_latency['LatencyShareBin'] = pd.cut(df_latency['Latency_share'], latency_bins,include_lowest=True).astype(str)\n",
    "\n",
    "display(pd.crosstab(df_latency['LatencyShareBin'],df_latency['Churn'],normalize='index',margins=True))\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___What you should see___:\n",
    "\n",
    "* Looks like most people have latency <5% of total traffic\n",
    "* For very small group, it goes up above 30%\n",
    "* Churn doubles for those with latency over 10% of traffi\n",
    "* Churn is nearly certain for those whose latency is over 30% of traffic\n",
    "\n",
    "__Takeaway - this a very good feature to add into the model__\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next-up, are people over-paying:\n",
    "\n",
    "* To do that - we need to: \n",
    " * figure out how much are people paying per month (total rev/tenure)\n",
    " * figure out how many services they have (Count phone, interent, tv)\n",
    " * figure out what is month charge/service look like\n",
    " * Remember, need to look at month-to-month folks only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overpayment = df.copy()\n",
    "\n",
    "\n",
    "# Apparently TotalRevnue cannot be downacasted to flaot by auto-magic so forcing it here\n",
    "df['TotalRevenue']= pd.to_numeric(df['TotalRevenue'], errors='coerce').fillna(0, downcast='infer')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___What you should see:___\n",
    "\n",
    "* Looks like folks with multiple services are in fact more likely to churn out, but slightly (crosstab)\n",
    "* When we plot average dollars paid per service per month we see that regardless of number of services, those with higher price paid are in fact somewhat more likely to churn-out.\n",
    "* That signal is not very strong, but we can hope that ML algo will be able to make it useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Features into training/test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df.copy()\n",
    "\n",
    "\n",
    "\n",
    "FEATURES = None\n",
    "\n",
    "# Set string containing column name that will be our target\n",
    "TARGET = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"__Orginal distribution__\"))\n",
    "display(pd.Categorical(df_transformed_target[TARGET]).describe())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# Lets verify that our splitting has the same distribution\n",
    "display(Markdown(\"__Split in training set__\"))\n",
    "display(pd.Categorical(y_train).describe())\n",
    "display(Markdown(\"__Split in test set__\"))\n",
    "display(pd.Categorical(y_test).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___What you should see___:\n",
    "\n",
    "* Latency share and AvgRevServicePerMonth are both part of feature set\n",
    "* We split training and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit a model\n",
    "XG_model = None \n",
    "\n",
    "\n",
    "# Get test predictions\n",
    "y_predict = None\n",
    "\n",
    "# Get test prediction probabilities\n",
    "y_predict_proba = None\n",
    "\n",
    "\n",
    "# Get accuracy score\n",
    "xg_accuracy = None\n",
    "display('XGBoost model test set accuracy: {:.4f}'.format(xg_accuracy))\n",
    "display('XGBoost model predicion distribution')\n",
    "display(pd.Categorical(y_predict).describe())\n",
    "\n",
    "# Display confusion matrix\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predict), \n",
    "             columns=['Predicted Not Churn', 'Predicted to Churn'],\n",
    "             index=['Actual Not Churn', 'Actual Churn']))\n",
    "\n",
    "# Calculate AUROC\n",
    "false_positive_rate, true_positive_rate, thresholds = None\n",
    "roc_auc = None\n",
    "display(\"auROC is: {}\".format(roc_auc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___What you should see___:\n",
    "\n",
    "* Test set accuracy improved a little bit to 93% from 91%\n",
    "* auROC improved a little bit too. Now at 94.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC chart\n",
    "plt.plot(false_positive_rate, true_positive_rate, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features': FEATURES, 'importance': XG_model.feature_importances_}).sort_values('importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___What you should see:___\n",
    "\n",
    "* In fact, we see newly engineered features rising to the top of the importance\n",
    " * AvgRevPerServicePerMonth and Latency_share are in top 5\n",
    "* Oddly - number of services is seemingly not relevant..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do a little bit of hyper-parameter tuning - let's play with depth only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try various models with different max_depth \n",
    "XG_model = None\n",
    "XG_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get predictions for test set\n",
    "y_predict = None\n",
    "\n",
    "# Get prediction probabilities for test set\n",
    "y_predict_proba = None\n",
    "\n",
    "# Calculate accuracy score\n",
    "xg_accuracy = None\n",
    "display('XGBoost model test set accuracy: {:.4f}'.format(xg_accuracy))\n",
    "display('XGBoost model predicion distribution')\n",
    "display(pd.Categorical(y_predict).describe())\n",
    "\n",
    "display(pd.DataFrame(confusion_matrix(y_test, y_predict), \n",
    "             columns=['Predicted Not Churn', 'Predicted to Churn'],\n",
    "             index=['Actual Not Churn', 'Actual Churn']))\n",
    "\n",
    "\n",
    "\n",
    "# Now, calculate and display AUROC\n",
    "false_positive_rate, true_positive_rate, thresholds = None\n",
    "roc_auc = None\n",
    "display(\"auROC is: {}\".format(roc_auc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "*Looks like max-depth of 5 raises auROC just a little bit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
